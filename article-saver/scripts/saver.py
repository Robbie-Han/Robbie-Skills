#!/usr/bin/env python3
"""
Article Saver - ä¸“æ³¨å¾®ä¿¡ã€Xã€çŸ¥ä¹çš„æ–‡ç« æŠ“å–å·¥å…·
åŠŸèƒ½ï¼š
1. è‡ªåŠ¨åˆ†å¹³å°å­˜æ”¾ï¼šç´ æ/{å¹³å°}/{æ—¥æœŸ}_{æ ‡é¢˜}/
2. ä¿æŒå›¾ç‰‡/GIFåŸç”»è´¨é‡
3. ä¸“æ³¨æ­£æ–‡å†…å®¹ï¼Œå‰”é™¤å†—ä½™å…ƒæ•°æ®
"""

import os
import sys
import json
import asyncio
import requests
import re
import shutil
from urllib.parse import urlparse
from pathlib import Path
from datetime import datetime
from playwright.async_api import async_playwright

# é…ç½®
DEFAULT_OUTPUT_ROOT = Path.home() / "Documents/WebContent/ç´ æ"
SCRIPT_DIR = Path(__file__).parent
SKILL_DIR = SCRIPT_DIR.parent
DATA_DIR = SKILL_DIR / "data"
WECHAT_AUTH_FILE = DATA_DIR / "wechat_auth.json"
ZHIHU_AUTH_FILE = DATA_DIR / "zhihu_auth.json"

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
DATA_DIR.mkdir(parents=True, exist_ok=True)

class ArticleSaver:
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.output_root = DEFAULT_OUTPUT_ROOT
        self.mobile_ua = 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.38(0x18002629) NetType/WIFI Language/zh_CN'
        self.desktop_ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

    def log(self, msg):
        if self.verbose:
            print(msg)

    def identify_platform(self, url):
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        path = parsed.path.lower()

        if 'mp.weixin.qq.com' in domain:
            return 'å¾®ä¿¡å…¬ä¼—å·', 'wechat'
        elif 'x.com' in domain or 'twitter.com' in domain:
            return 'X', 'x'
        elif 'zhihu.com' in domain:
            if 'zhuanlan' in domain:
                return 'çŸ¥ä¹ä¸“æ ', 'zhihu'
            return 'çŸ¥ä¹', 'zhihu'
        return 'å…¶ä»–', 'other'

    def sanitize_filename(self, name, max_length=50):
        name = re.sub(r'[<>:"/\\|?*\n\r\t]', '', name)
        name = re.sub(r'\s+', ' ', name).strip()
        if len(name) > max_length:
            name = name[:max_length]
        return name or "untitled"

    def read_with_jina(self, url):
        try:
            jina_url = f"https://r.jina.ai/{url}"
            headers = {
                'Accept': 'text/markdown',
                'User-Agent': self.desktop_ua
            }
            response = requests.get(jina_url, headers=headers, timeout=30)
            if response.status_code == 200:
                content = response.text
                if 'ç¯å¢ƒå¼‚å¸¸' in content or 'å®ŒæˆéªŒè¯' in content or '403 Forbidden' in content:
                    return {'success': False, 'error': 'Jina ä¹Ÿæ— æ³•ç»•è¿‡éªŒè¯'}
                return {'success': True, 'content': content}
            return {'success': False, 'error': f'HTTP {response.status_code}'}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def extract_title_from_content(self, content):
        match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if match:
            return match.group(1).strip()
        return "Unknown_Title"

    def extract_images_from_content(self, content):
        return re.findall(r'!\[.*?\]\((https?://[^\s\)]+)\)', content)

    def download_image_requests(self, url, save_dir, index):
        try:
            headers = {'User-Agent': self.desktop_ua}
            # å¤„ç†é˜²ç›—é“¾
            if 'zhihu.com' in url or 'zhimg.com' in url:
                headers['Referer'] = 'https://www.zhihu.com/'

            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '')
                ext = '.jpg'
                if 'png' in content_type: ext = '.png'
                elif 'gif' in content_type: ext = '.gif'
                elif 'webp' in content_type: ext = '.webp'

                filename = f"img_{index:02d}{ext}"
                filepath = save_dir / filename
                filepath.write_bytes(response.content)
                return filename
        except Exception:
            pass
        return None

    async def scrape(self, url):
        platform_name, platform_id = self.identify_platform(url)
        self.log(f"ğŸ“ ç›®æ ‡å¹³å°: {platform_name}")

        # å¦‚æœçŸ¥ä¹è¢«é™åˆ¶ï¼Œå°è¯•ä½¿ç”¨ Jina Reader ä½œä¸ºå¤‡é€‰
        if platform_id == 'zhihu':
            self.log("ğŸ”„ çŸ¥ä¹å¹³å°å°è¯•ä½¿ç”¨ Jina Reader ç­–ç•¥...")
            jina_data = self.read_with_jina(url)
            if jina_data.get('success'):
                # æ„é€ ç¬¦åˆ extract_content ç»“æ„çš„æ•°æ®
                title = self.extract_title_from_content(jina_data['content'])
                image_urls = self.extract_images_from_content(jina_data['content'])
                data = {
                    'title': title,
                    'author': 'çŸ¥ä¹ç”¨æˆ·',
                    'content': jina_data['content'],
                    'image_urls': image_urls
                }

                # Jina è¿”å›çš„ markdown ä¸­å›¾ç‰‡å·²ç»æ˜¯ ![...](url) æ ¼å¼
                # æˆ‘ä»¬éœ€è¦ä¸‹è½½è¿™äº›å›¾ç‰‡å¹¶æ›¿æ¢æœ¬åœ°è·¯å¾„

                # è·¯å¾„è§„åˆ’
                date_str = datetime.now().strftime("%Y-%m-%d")
                title_sanitized = self.sanitize_filename(title)
                folder_name = f"{date_str}_{title_sanitized}"
                save_dir = self.output_root / platform_name / folder_name
                save_dir.mkdir(parents=True, exist_ok=True)

                downloaded = []
                for i, img_url in enumerate(image_urls):
                    self.log(f"  â¬‡ï¸ ä¸‹è½½å›¾ç‰‡ [{i+1}/{len(image_urls)}]: {img_url[:50]}...")
                    local_name = self.download_image_requests(img_url, save_dir, i)
                    if local_name:
                        downloaded.append({"index": i, "filename": local_name, "temp_path": str(save_dir / local_name), "original_url": img_url})

                # æ›´æ–°å†…å®¹ä¸­çš„å›¾ç‰‡å¼•ç”¨
                content = data['content']
                for img_info in downloaded:
                    # Jina è¿”å›çš„ markdown å›¾ç‰‡æ ¼å¼é€šå¸¸æ˜¯ ![alt](url)
                    # æˆ‘ä»¬ç®€å•æ›¿æ¢ url ä¸ºæœ¬åœ°æ–‡ä»¶å
                    content = content.replace(img_info['original_url'], img_info['filename'])

                data['content'] = content

                # ç›´æ¥ä¿å­˜ï¼Œä¸éœ€è¦å†èµ° self.save çš„ç§»åŠ¨é€»è¾‘ï¼Œå› ä¸ºå·²ç»ä¸‹è½½åˆ°ç›®æ ‡ç›®å½•äº†
                meta = f"""---
title: {data['title']}
author: {data['author']}
platform: {platform_name}
url: {url}
saved_at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
---

"""
                md_file = save_dir / "content.md"
                md_file.write_text(meta + content, encoding='utf-8')
                self.log(f"\nâœ… å·²ä¿å­˜è‡³: {save_dir}")

                return {"success": True, "data": data, "platform_name": platform_name, "platform_id": platform_id}

        async with async_playwright() as p:
            # é’ˆå¯¹ä¸åŒå¹³å°é€‰æ‹© UA
            ua = self.mobile_ua if platform_id == 'wechat' else self.desktop_ua
            browser = await p.chromium.launch(headless=True)

            # åŠ è½½ç™»å½•æ€
            context_args = {"user_agent": ua}
            if platform_id == 'wechat' and WECHAT_AUTH_FILE.exists():
                context_args["storage_state"] = str(WECHAT_AUTH_FILE)
            elif platform_id == 'zhihu':
                if ZHIHU_AUTH_FILE.exists():
                    context_args["storage_state"] = str(ZHIHU_AUTH_FILE)
                    self.log("ğŸ”‘ å·²åŠ è½½çŸ¥ä¹ç™»å½•æ€")
                else:
                    self.log("âš ï¸ æœªæ£€æµ‹åˆ°çŸ¥ä¹ç™»å½•æ€ (data/zhihu_auth.json)ï¼Œå¯èƒ½ä¼šè§¦å‘åçˆ¬éªŒè¯")

            context = await browser.new_context(**context_args)
            page = await context.new_page()

            # é’ˆå¯¹çŸ¥ä¹çš„ç‰¹æ®Šå¤„ç†ï¼šçŸ¥ä¹å¯èƒ½ä¼šæœ‰å¼ºåçˆ¬
            try:
                self.log(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)

                # ç­‰å¾…å¹¶å¤„ç†å¯èƒ½çš„é‡å®šå‘æˆ–åçˆ¬
                if platform_id == 'zhihu':
                    await page.wait_for_timeout(3000)
                    if "liantong" in page.url or "captcha" in page.url:
                        self.log("âš ï¸ æ£€æµ‹åˆ°çŸ¥ä¹éªŒè¯ç æˆ–è·³è½¬")

                await page.wait_for_timeout(2000)
            except Exception as e:
                self.log(f"âš ï¸ é¡µé¢åŠ è½½å¼‚å¸¸: {str(e)}")

            # è°ƒè¯•ï¼šæ‰“å°é¡µé¢æ ‡é¢˜å’Œå½“å‰ URL
            page_title = await page.title()
            current_url = page.url
            self.log(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")
            self.log(f"ğŸ”— å½“å‰ URL: {current_url}")

            # æå–é€»è¾‘
            data = await self.extract_content(page, platform_id)

            if not data or not data.get('content'):
                # è®°å½•å¤±è´¥æ—¶çš„ HTML ç‰‡æ®µ
                html = await page.content()
                self.log(f"âš ï¸ æå–å¤±è´¥ã€‚é¡µé¢ HTML é•¿åº¦: {len(html)}")
                if len(html) > 0:
                    self.log(f"âš ï¸ HTML å‰ 500 å­—: {html[:500]}")

                await browser.close()
                return {"success": False, "error": f"æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹ (æ ‡é¢˜: {page_title}, URL: {current_url})"}

            # ä¸‹è½½å›¾ç‰‡
            downloaded_images = await self.download_images(page, data['image_urls'], platform_id)
            data['downloaded_images'] = downloaded_images

            await browser.close()
            return {"success": True, "data": data, "platform_name": platform_name, "platform_id": platform_id}

    async def extract_content(self, page, platform_id):
        if platform_id == 'wechat':
            return await page.evaluate("""
                () => {
                    const title = document.querySelector('#activity-name')?.innerText?.trim() || '';
                    const author = document.querySelector('#js_name')?.innerText?.trim() || '';
                    const contentEl = document.querySelector('#js_content');
                    if (!contentEl) return null;

                    const image_urls = [];
                    const imgs = contentEl.querySelectorAll('img');
                    imgs.forEach((img, i) => {
                        let src = img.getAttribute('data-src') || img.getAttribute('src');
                        if (src && src.startsWith('http')) {
                            image_urls.push(src);
                            const placeholder = document.createTextNode(`{{IMG_${i}}}`);
                            img.parentNode.replaceChild(placeholder, img);
                        }
                    });

                    let markdown = contentEl.innerHTML
                        .replace(/<h[1-6][^>]*>(.*?)<\\/h[1-6]>/gi, (m, c) => `\\n## ${c.replace(/<[^>]+>/g, '')}\\n`)
                        .replace(/<p[^>]*>/gi, '\\n')
                        .replace(/<br[^>]*>/gi, '\\n')
                        .replace(/<strong[^>]*>(.*?)<\\/strong>/gi, '**$1**')
                        .replace(/<[^>]+>/g, '')
                        .replace(/\\n{3,}/g, '\\n\\n')
                        .trim();

                    return { title, author, content: markdown, image_urls };
                }
            """)
        elif platform_id == 'zhihu':
            return await page.evaluate("""
                () => {
                    let title = document.querySelector('.QuestionHeader-title')?.innerText ||
                                document.querySelector('.Post-Title')?.innerText ||
                                document.querySelector('h1')?.innerText || '';
                    let author = document.querySelector('.AuthorInfo-name')?.innerText ||
                                 document.querySelector('.UserLink-link')?.innerText || '';

                    // çŸ¥ä¹å›ç­”ã€æ–‡ç« æˆ–ä¸“æ æ­£æ–‡
                    let contentEl = document.querySelector('.RichText.ztext') ||
                                    document.querySelector('.Post-RichTextContainer') ||
                                    document.querySelector('.Post-Content') ||
                                    document.querySelector('article');

                    if (!contentEl) return null;

                    const image_urls = [];
                    const imgs = contentEl.querySelectorAll('img');
                    imgs.forEach((img, i) => {
                        let src = img.getAttribute('data-actualsrc') ||
                                 img.getAttribute('data-original') ||
                                 img.getAttribute('src');
                        if (src && src.startsWith('http')) {
                            // ç§»é™¤åŠ¨æ€å°ºå¯¸å‚æ•°ï¼Œè·å–åŸå›¾
                            if (src.includes('?')) {
                                src = src.split('?')[0];
                            }
                            image_urls.push(src);
                            const placeholder = document.createTextNode(`{{IMG_${i}}}`);
                            img.parentNode.replaceChild(placeholder, img);
                        }
                    });

                    let markdown = contentEl.innerText.replace(/\\n{3,}/g, '\\n\\n').trim();
                    return { title, author, content: markdown, image_urls };
                }
            """)
        elif platform_id == 'x':
            # X çš„é€»è¾‘ï¼šå°è¯•æŠ“å–ä¸»æ¨æ–‡å†…å®¹æˆ–é•¿æ–‡å†…å®¹
            return await page.evaluate("""
                () => {
                    // 1. æŸ¥æ‰¾æ¨æ–‡æˆ–é•¿æ–‡å®¹å™¨
                    const tweet = document.querySelector('article[data-testid="tweet"]') ||
                                  document.querySelector('article[role="article"]') ||
                                  document.querySelector('main article');
                    if (!tweet) return null;

                    // 2. æå–ä½œè€…
                    const authorEl = document.querySelector('div[data-testid="User-Name"]') ||
                                     document.querySelector('div[data-testid="AuthorInfo-name"]');
                    const author = authorEl?.innerText?.split('\\n')[0] || 'X_User';

                    // 3. æå–å†…å®¹ï¼ˆæ”¯æŒæ™®é€šæ¨æ–‡å’Œé•¿æ–‡ï¼‰
                    const textEl = tweet.querySelector('div[data-testid="tweetText"]') ||
                                   tweet.querySelector('div[data-testid="articleBody"]') ||
                                   tweet;

                    // 4. æå–æ ‡é¢˜
                    let title = '';
                    const articleTitleEl = tweet.querySelector('div[data-testid="articleTitle"]');
                    if (articleTitleEl) {
                        title = articleTitleEl.innerText;
                    } else {
                        // å¦‚æœæ²¡æœ‰ä¸“é—¨çš„æ ‡é¢˜ï¼Œä½¿ç”¨å†…å®¹å‰30ä¸ªå­—ç¬¦
                        const text = textEl?.innerText || '';
                        title = text.slice(0, 30).replace(/\\n/g, ' ') || 'X_Post';
                    }

                    // 5. æå–å›¾ç‰‡
                    const image_urls = [];
                    // æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡å®¹å™¨
                    const photoEls = tweet.querySelectorAll('div[data-testid="tweetPhoto"] img, div[data-testid="articleImage"] img, img[src*="/media/"], img[src*="pbs.twimg.com/media"]');

                    photoEls.forEach((img, i) => {
                        let src = img.getAttribute('src');
                        if (src) {
                            // è½¬æ¢ä¸ºåŸå›¾é“¾æ¥
                            if (src.includes('format=')) {
                                // å¤„ç†ç±»ä¼¼ https://pbs.twimg.com/media/xxxx?format=jpg&name=small çš„é“¾æ¥
                                src = src.replace(/name=[a-zA-Z0-9_]+/, 'name=large');
                            } else if (src.includes('pbs.twimg.com/media')) {
                                // å¦‚æœæ²¡æœ‰ name å‚æ•°ï¼Œè¿½åŠ 
                                if (!src.includes('name=')) {
                                    src += '&name=large';
                                }
                            }

                            // é¿å…é‡å¤æ·»åŠ 
                            if (!image_urls.includes(src)) {
                                image_urls.push(src);
                                // å°è¯•æ›¿æ¢ DOM ä¸­çš„å›¾ç‰‡ä¸ºå ä½ç¬¦ï¼ˆä»…å¯¹å¯è§å›¾ç‰‡ï¼‰
                                try {
                                    const placeholder = document.createTextNode(`{{IMG_${image_urls.length - 1}}}`);
                                    if (img.parentNode) {
                                        img.parentNode.replaceChild(placeholder, img);
                                    }
                                } catch (e) {}
                            }
                        }
                    });

                    let markdown = textEl?.innerText || '';
                    return { title, author, content: markdown, image_urls };
                }
            """)
        return None

    async def download_images(self, page, urls, platform_id):
        downloaded = []
        import tempfile
        temp_dir = Path(tempfile.gettempdir()) / "article_saver_images"
        temp_dir.mkdir(parents=True, exist_ok=True)

        for i, url in enumerate(urls):
            try:
                self.log(f"  â¬‡ï¸ ä¸‹è½½å›¾ç‰‡ [{i+1}/{len(urls)}]: {url[:50]}...")

                # åœ¨æµè§ˆå™¨ç¯å¢ƒä¸‹ä¸‹è½½ä»¥è·å–åŸå›¾ï¼ˆå¤„ç† referer/cookiesï¼‰
                img_data = await page.evaluate("""
                    async (url) => {
                        try {
                            const response = await fetch(url);
                            const blob = await response.blob();
                            return await new Promise((resolve, reject) => {
                                const reader = new FileReader();
                                reader.onloadend = () => {
                                    const base64data = reader.result.split(',')[1];
                                    resolve({ success: true, data: base64data, type: blob.type });
                                };
                                reader.onerror = reject;
                                reader.readAsDataURL(blob);
                            });
                        } catch (e) {
                            return { success: false, error: e.toString() };
                        }
                    }
                """, url)

                if img_data and img_data.get('success'):
                    import base64
                    content_type = img_data.get('type', 'image/jpeg')
                    ext = '.jpg'
                    if 'png' in content_type: ext = '.png'
                    elif 'gif' in content_type: ext = '.gif'
                    elif 'webp' in content_type: ext = '.webp'

                    filename = f"img_{i:02d}{ext}"
                    filepath = temp_dir / filename
                    filepath.write_bytes(base64.b64decode(img_data['data']))

                    downloaded.append({
                        "index": i,
                        "filename": filename,
                        "temp_path": str(filepath)
                    })
            except Exception as e:
                self.log(f"  âŒ ä¸‹è½½å¤±è´¥: {str(e)}")

        return downloaded

    def save(self, scrape_result, url):
        data = scrape_result['data']
        platform_name = scrape_result['platform_name']

        # è·¯å¾„è§„åˆ’: {ROOT}/{Platform}/{Date}_{Title}/
        date_str = datetime.now().strftime("%Y-%m-%d")
        title = self.sanitize_filename(data['title'])
        folder_name = f"{date_str}_{title}"

        save_dir = self.output_root / platform_name / folder_name
        save_dir.mkdir(parents=True, exist_ok=True)

        # ç§»åŠ¨å›¾ç‰‡
        for img_info in data['downloaded_images']:
            src = Path(img_info['temp_path'])
            dst = save_dir / img_info['filename']
            if src.exists():
                shutil.move(str(src), str(dst))

        # å¤„ç† Markdown ä¸­çš„å›¾ç‰‡å¼•ç”¨
        content = data['content']
        for img_info in data['downloaded_images']:
            placeholder = f"{{{{IMG_{img_info['index']}}}}}"
            content = content.replace(placeholder, f"\n![å›¾ç‰‡]({img_info['filename']})\n")

        # å†™å…¥æ–‡ä»¶
        meta = f"""---
title: {data['title']}
author: {data['author']}
platform: {platform_name}
url: {url}
saved_at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
---

"""
        md_file = save_dir / "content.md"
        md_file.write_text(meta + content, encoding='utf-8')

        self.log(f"\nâœ… å·²ä¿å­˜è‡³: {save_dir}")
        return str(save_dir)

async def main():
    if len(sys.argv) < 2:
        print("Usage: python saver.py <url>")
        return

    url = sys.argv[1]
    saver = ArticleSaver()
    result = await saver.scrape(url)

    if result.get('success'):
        saver.save(result, url)
    else:
        print(f"âŒ æŠ“å–å¤±è´¥: {result.get('error')}")

if __name__ == "__main__":
    asyncio.run(main())
